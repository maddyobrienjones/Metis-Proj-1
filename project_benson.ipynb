{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA Continued: Challenge Set 1 Review, Data Checking & Cleaning with Pandas and Viz Tools\n",
    "\n",
    "Aka, learning how to always expect your data to have more problems.\n",
    "\n",
    "**Learning Goals**:\n",
    "\n",
    "1. Review core pandas methods and understand their application to challenge set 1\n",
    "2. Understand basic methods for data quality checking and cleaning \n",
    "3. See examples of how to use visualization as an aid in exploring data quality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1\n",
    "\n",
    "- Open up a new IPython notebook\n",
    "- Download a few MTA turnstile data files\n",
    "- Read them into a pandas dataframe (pd.read_csv()), format the columns nicely, and display the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: http://web.mta.info/developers/turnstile.html\n",
    "def get_data(week_nums):\n",
    "    url = \"http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt\"\n",
    "    dfs = []\n",
    "    for week_num in week_nums:\n",
    "        file_url = url.format(week_num)\n",
    "        dfs.append(pd.read_csv(file_url))\n",
    "    return pd.concat(dfs)\n",
    "        \n",
    "week_nums = [180505]\n",
    "turnstiles_df = get_data(week_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the column names make any sense? sanity checking the column name\n",
    "turnstiles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df.columns = [column.strip() for column in turnstiles_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three weeks of Data\n",
    "#This is to make sure we have 3 weeks worth of data\n",
    "turnstiles_df.DATE.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the date and time fields into a single datetime column\n",
    "turnstiles_df[\"DATE_TIME\"] = pd.to_datetime(turnstiles_df.DATE + \" \" + turnstiles_df.TIME, \n",
    "                                            format=\"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need the below values to specify a specific turnstile. \n",
    "\n",
    "mask = ((turnstiles_df[\"C/A\"] == \"A002\") & \n",
    "        (turnstiles_df[\"UNIT\"] == \"R051\") & \n",
    "        (turnstiles_df[\"SCP\"] == \"02-00-00\") & \n",
    "        (turnstiles_df[\"STATION\"] == \"59 ST\"))\n",
    "#mask, this returns true and false for all rows in the data frame. \n",
    "turnstiles_df[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((turnstiles_df[\"C/A\"] == \"R626\") & \n",
    "(turnstiles_df[\"UNIT\"] == \"R062\") & \n",
    "(turnstiles_df[\"SCP\"] == \"00-00-00\") & \n",
    "(turnstiles_df[\"STATION\"] == \"CROWN HTS-UTICA\"))\n",
    "turnstiles_df[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df = turnstiles_df.sort_values([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check to verify that \"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\" is unique\n",
    "(turnstiles_df\n",
    " .groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"])\n",
    " .ENTRIES.count()\n",
    " .reset_index()\n",
    " .sort_values(\"ENTRIES\", ascending=False)).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On 9/16, we seem to have two entries for same time.  Let's take a look\n",
    "mask = ((turnstiles_df[\"C/A\"] == \"R504\") & \n",
    "(turnstiles_df[\"UNIT\"] == \"R276\") & \n",
    "(turnstiles_df[\"SCP\"] == \"00-00-01\") & \n",
    "(turnstiles_df[\"STATION\"] == \"VERNON-JACKSON\") &\n",
    "(turnstiles_df[\"DATE_TIME\"].dt.date == datetime.datetime(2016, 9, 16).date()))\n",
    "turnstiles_df[mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df.DESC.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of the duplicate entry\n",
    "turnstiles_df.sort_values([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"], \n",
    "                          inplace=True, ascending=False)\n",
    "turnstiles_df.drop_duplicates(subset=[\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check to verify that \"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\" is unique\n",
    "# check there are no other duplicates for all data sets \n",
    "(turnstiles_df\n",
    " .groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"])\n",
    " .EXITS.count()\n",
    " .reset_index()\n",
    " .sort_values(\"EXITS\", ascending=False)).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df[[\"PREV_DATE_TIME\", \"PREV_ENTRIES\", \"PREV_EXITS\"]] = (turnstiles_df\n",
    "                                                       .groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\"])[\"DATE_TIME\", \"ENTRIES\", \"EXITS\"]\n",
    "                                                       .transform(lambda grp: grp.shift(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows for the earliest date in the df\n",
    "turnstiles_df.dropna(subset=[\"PREV_DATE_TIME\"], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df[turnstiles_df[\"ENTRIES\"] < turnstiles_df[\"PREV_ENTRIES\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many stations have this problem\n",
    "\n",
    "(turnstiles_df[turnstiles_df[\"ENTRIES\"] < turnstiles_df[\"PREV_ENTRIES\"]]\n",
    "    .groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\"])\n",
    "    .size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_interval_counts(row, max_counter, column):\n",
    "    counter = row[column] - row[\"PREV_\"+column]\n",
    "    if counter < 0:\n",
    "        counter = -counter\n",
    "    if counter > max_counter:\n",
    "        print(row[column], row[\"PREV_\"+column])\n",
    "        return 0\n",
    "    return counter\n",
    "\n",
    "# If counter is > 1Million, then the counter might have been reset.  \n",
    "# Just set it to zero as different counters have different cycle limits\n",
    "_ = turnstiles_df.apply(get_hourly_interval_counts, axis=1, max_counter=1000000, column = \"EXITS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_interval_counts(row, max_counter, column):\n",
    "    counter = row[column] - row[\"PREV_\"+column]\n",
    "    if counter < 0:\n",
    "        # Maybe counter is reversed?\n",
    "        counter = -counter\n",
    "    if counter > max_counter:\n",
    "        # Maybe counter was reset to 0? \n",
    "        print(row[column], row[\"PREV_\"+column])\n",
    "        counter = min(row[column], row[\"PREV_\"+column])\n",
    "    if counter > max_counter:\n",
    "        # Check it again to make sure we're not still giving a counter that's too big\n",
    "        return 0\n",
    "    return counter\n",
    "\n",
    "# If counter is > 1Million, then the counter might have been reset.  \n",
    "# Just set it to zero as different counters have different cycle limits\n",
    "# It'd probably be a good idea to use a number even significantly smaller than 1 million as the limit!\n",
    "turnstiles_df[\"INTERVAL_ENTRIES\"] = turnstiles_df.apply(get_hourly_interval_counts, axis=1, max_counter=1000000, column=\"ENTRIES\")\n",
    "turnstiles_df[\"INTERVAL_EXITS\"] = turnstiles_df.apply(get_hourly_interval_counts, axis=1, max_counter=1000000, column=\"EXITS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df[\"COMMUTE_INTERVAL\"] = (turnstiles_df[\"DATE_TIME\"]\n",
    "                                                       .transform(lambda t: (t.hour >= 10 and\n",
    "                                                                                 t.hour <= 14)+2*(t.hour >= 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df[turnstiles_df.COMMUTE_INTERVAL==1][\"INTERVAL_ENTRIES\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df.loc[turnstiles_df.INTERVAL_ENTRIES==35112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstiles_df[turnstiles_df.DIVISION==\"PTH\"][\"STATION\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turnstile_df = turnstiles_df.drop(turnstiles_df.STATION.isin[['NEWARK HW BMEBE', 'HARRISON', 'JOURNAL SQUARE', 'GROVE STREET',\n",
    "#        'EXCHANGE PLACE', 'PAVONIA/NEWPORT', 'CITY / BUS',\n",
    "#        'CHRISTOPHER ST', '9TH STREET', '14TH STREET', 'TWENTY THIRD ST',\n",
    "#        'THIRTY ST', 'LACKAWANNA', 'THIRTY THIRD ST', 'NEWARK BM BW',\n",
    "#        'NEWARK C', 'NEWARK HM HE', 'PATH WTC 2', 'PATH NEW WTC']])\n",
    "\n",
    "newList = ['NEWARK HW BMEBE', 'HARRISON', 'JOURNAL SQUARE', 'GROVE STREET', 'EXCHANGE PLACE', 'PAVONIA/NEWPORT', 'CITY / BUS', 'LACKAWANNA', 'NEWARK BM BW',\n",
    "       'NEWARK C', 'NEWARK HM HE']\n",
    "new_turnstile_df = turnstiles_df[~turnstiles_df['STATION'].isin(newList)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_turnstile_df[new_turnstile_df.DIVISION==\"PTH\"][\"STATION\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_turnstile_df[new_turnstile_df.COMMUTE_INTERVAL==1][\"INTERVAL_ENTRIES\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_turnstile_df.loc[new_turnstile_df.INTERVAL_ENTRIES==3006]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get daily entries by station\n",
    "stations_Interval_Entries = \\\n",
    "(new_turnstile_df[new_turnstile_df[\"COMMUTE_INTERVAL\"] == 1].groupby(['STATION'])['INTERVAL_ENTRIES'].mean()\n",
    "                 .reset_index()).sort_values(by=\"INTERVAL_ENTRIES\", ascending=False)  \n",
    "\n",
    "stations_Interval_Exits = \\\n",
    "(new_turnstile_df[new_turnstile_df[\"COMMUTE_INTERVAL\"] == 2].groupby(['STATION'])['INTERVAL_EXITS'].mean()\n",
    "                 .reset_index()).sort_values(by=\"INTERVAL_EXITS\", ascending=False) \n",
    "\n",
    "\n",
    "stations_Interval_Entries.describe()\n",
    "#stations_Interval_Exits.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_Interval_Entries = \\\n",
    "(new_turnstile_df.groupby(['STATION'])['INTERVAL_ENTRIES'].mean()\n",
    "                 .reset_index()).sort_values(by=\"INTERVAL_ENTRIES\", ascending=False)  \n",
    "\n",
    "stations_Interval_Exits = \\\n",
    "(new_turnstile_df.groupby(['STATION'])['INTERVAL_EXITS'].mean()\n",
    "                 .reset_index()).sort_values(by=\"INTERVAL_EXITS\", ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(new_turnstile_df['INTERVAL_ENTRIES']\n",
    "             [new_turnstile_df['INTERVAL_ENTRIES'] < 10000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(new_turnstile_df['INTERVAL_EXITS']\n",
    "             [new_turnstile_df['INTERVAL_EXITS'] < 10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then get top 10 stations by daily volume \n",
    "# (sum across all days is a reasonable way to define this)\n",
    "top10_stations = \\\n",
    "    (stations_daily.groupby(['STATION'])['DAILY_ENTRIES'].sum()\n",
    "                   .reset_index()\n",
    "                   .sort_values(by='DAILY_ENTRIES',ascending=False) \n",
    "                   .STATION.head(10))\n",
    "\n",
    "top10_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next create a new df that filters the stations daily data down\n",
    "# to the top 10 stations\n",
    "stations_daily_top10 = \\\n",
    "    stations_daily[stations_daily['STATION'].isin(top10_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use seaborn to create a boxplot by station\n",
    "sns.boxplot('DAILY_ENTRIES', 'STATION', data=stations_daily_top10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
